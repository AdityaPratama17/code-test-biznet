{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3c694db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "from typing import Sequence\n",
    "\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e96a5",
   "metadata": {},
   "source": [
    "## Simple ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e00551",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    temperature=0.2,\n",
    "    model='gemma3:1b'\n",
    ")\n",
    "\n",
    "# -- create prompt template\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Please response to the user queries\"),\n",
    "        (\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -- create output parser\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "# -- create chain\n",
    "chain=prompt|llm|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d76b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! Iâ€™m doing well, thanks for asking. As a large language model, I donâ€™t really *feel* things, but Iâ€™m functioning perfectly and ready to help you with whatever you need. ðŸ˜Š How about you?\n"
     ]
    }
   ],
   "source": [
    "input_text = \"hi, how are you?\"\n",
    "response = chain.invoke({'question':input_text})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84850dbb",
   "metadata": {},
   "source": [
    "## Advance ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca27cb4",
   "metadata": {},
   "source": [
    "### Without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b10871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load llm\n",
    "llm = ChatOllama(\n",
    "    temperature=0.2,\n",
    "    model='gemma3:1b'\n",
    ")\n",
    "\n",
    "# create prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Please response to the user queries.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create trimmer\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=1000,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary for state  \n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "        \n",
    "# create node function\n",
    "def call_model(state: State):\n",
    "    # lakukan trimmer pada history message\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    \n",
    "    # masukan trimmed message dan language ke prompt template \n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages}\n",
    "    )\n",
    "\n",
    "    # generate jawaban dengan model LLM\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# buat workflow\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# -- compile workflow dengan memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- set config for memory state\n",
    "# tiap state pada memory akan dibedakan berdasarkan thread_id nya\n",
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "\n",
    "output = app.invoke({\n",
    "    \"messages\": [HumanMessage(\"hi my name is Bob\")], \n",
    "}, config)\n",
    "\n",
    "output = app.invoke({\n",
    "    \"messages\": [HumanMessage(\"what is my name?\")], \n",
    "}, config)\n",
    "\n",
    "output = app.invoke({\n",
    "    \"messages\": [HumanMessage(\"what your name?\")], \n",
    "}, config)\n",
    "\n",
    "output = app.invoke({\n",
    "    \"messages\": [HumanMessage(\"what python? in 1 paragraph\")], \n",
    "}, config)\n",
    "\n",
    "output = app.invoke({\n",
    "    \"messages\": [HumanMessage(\"what html? in 1 paragraph\")], \n",
    "}, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ab2289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi my name is Bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! Itâ€™s nice to meet you. How can I help you today? ðŸ˜Š\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! ðŸ˜Š \n",
      "\n",
      "Is there anything youâ€™d like to do with that information?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what your name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As a large language model, I don't have a name in the way a person does. I was created by Google! \n",
      "\n",
      "But you could sayâ€¦ **Google AI**! ðŸ˜„\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what python? in 1 paragraph\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Python is a versatile and widely-used high-level programming language known for its readability and ease of use. Itâ€™s designed to be beginner-friendly while still offering powerful capabilities for complex projects. Python excels in areas like data science, machine learning, web development, scripting, and automation, thanks to its extensive libraries and frameworks like NumPy and Pandas. Essentially, itâ€™s a great choice for anyone who wants to learn to code and tackle a wide range of tasks.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what html? in 1 paragraph\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "HTML (HyperText Markup Language) is the standard markup language for creating web pages. It provides the structure and content of a webpage, including text, images, links, and other elements. Think of it as the foundation â€“ it defines what elements are on the page and how they relate to each other. HTML uses tags to organize these elements, allowing web developers to build visually appealing and informative websites.\n"
     ]
    }
   ],
   "source": [
    "for chat in output[\"messages\"]:\n",
    "    chat.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67561c09",
   "metadata": {},
   "source": [
    "### With RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d79a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- load document\n",
    "# bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "#     bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "# )\n",
    "loader = PyPDFLoader('../resources/ISP Company FAQ.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "# -- split docs\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# -- load to vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(), # vector db hanya akan disimpan di dalam memory selama runtime\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# -- Create retriever\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33ba589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load llm\n",
    "llm = ChatOllama(\n",
    "    temperature=0.2,\n",
    "    model='gemma3:1b'\n",
    ")\n",
    "\n",
    "# create prompt template with document retrieval\n",
    "prompt_template_retrieval = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "                You are an assistant for question-answering tasks. \n",
    "                Use the following pieces of retrieved context to answer the question. \n",
    "                If you don't know the answer, say that you don't know. \n",
    "                Use three sentences maximum and keep the answer concise.\n",
    "                \n",
    "                retrieved context:\n",
    "                {context}\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create trimmer\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=1000,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7660c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary for state  \n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    # language: str\n",
    "        \n",
    "# create node function\n",
    "def call_model(state: State):\n",
    "    # lakukan trimmer pada history message\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    context = retriever.invoke(last_message.content)\n",
    "    \n",
    "    # masukan trimmed message dan language ke prompt template \n",
    "    prompt = prompt_template_retrieval.invoke({\n",
    "        \"context\": context,\n",
    "        \"messages\": trimmed_messages,\n",
    "    })\n",
    "\n",
    "    # generate jawaban dengan model LLM\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# buat workflow\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# -- compile workflow dengan memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33813043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi my name is Bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! How can I help you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can I cancel my order\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you can cancel your order. Orders can be canceled within 24 hours of placement. After that, cancellations are not guaranteed.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "whats my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I understand youâ€™re looking for information about your name. However, Iâ€™m designed to help with questions and provide assistance, and I donâ€™t have access to personal information like names. \n",
      "\n",
      "Perhaps you were thinking of another system or platform?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "\n",
    "output = app.invoke({\n",
    "    \"messages\": [HumanMessage(\"whats my name?\")], \n",
    "}, config)\n",
    "\n",
    "for chat in output[\"messages\"]:\n",
    "    chat.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf5af1",
   "metadata": {},
   "source": [
    "## Class Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee87988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary for state  \n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    \n",
    "\n",
    "class AdvanceChatBot:\n",
    "    def __init__(self):\n",
    "        self.vector_store = self.__init_vector_store()\n",
    "        self.text_splitter = self.__init_text_splitter()\n",
    "        self.llm = self.__init_llm_model()\n",
    "        self.trimmer = self.__init_trimmer()\n",
    "        self.prompt_template = self.__init_prompt_template()\n",
    "        self.prompt_template_with_retrieval = self.__init_prompt_template_with_retrieval()\n",
    "\n",
    "        self.is_use_rag = False\n",
    "    \n",
    "    def __init_vector_store(self):\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "        vector_store = FAISS(\n",
    "            embedding_function=embeddings,\n",
    "            index=index,\n",
    "            docstore=InMemoryDocstore(), # vector db hanya akan disimpan di dalam memory selama runtime\n",
    "            index_to_docstore_id={},\n",
    "        )\n",
    "        return vector_store\n",
    "    \n",
    "    def __init_text_splitter(self):\n",
    "        return RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,  # chunk size (characters)\n",
    "            chunk_overlap=200,  # chunk overlap (characters)\n",
    "            add_start_index=True,  # track index in original document\n",
    "        )\n",
    "\n",
    "    def __init_llm_model(self):\n",
    "        # load the LLM model\n",
    "        return ChatOllama(\n",
    "            temperature=0.5,\n",
    "            model='gemma3:1b'\n",
    "        )\n",
    "    \n",
    "    def __init_trimmer(self):\n",
    "        # create trimmer\n",
    "        return trim_messages(\n",
    "            max_tokens=1000,\n",
    "            strategy=\"last\",\n",
    "            token_counter=self.llm,\n",
    "            include_system=True,\n",
    "            allow_partial=False,\n",
    "            start_on=\"human\",\n",
    "        )\n",
    "    \n",
    "    def __init_prompt_template(self):\n",
    "        # create prompt template\n",
    "        return ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"You are a helpful assistant.\"\n",
    "                ),\n",
    "                MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def __init_prompt_template_with_retrieval(self):\n",
    "        return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"\n",
    "                    You are an assistant for question-answering tasks. \n",
    "                    Use the following pieces of retrieved context to answer the question. \n",
    "                    If you don't know the answer, say that you don't know. \n",
    "                    Use three sentences maximum and keep the answer concise.\n",
    "                    \n",
    "                    retrieved context:\n",
    "                    {context}\n",
    "                \"\"\"\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    def load_document(self, docs_path):\n",
    "        # load document\n",
    "        loader = PyPDFLoader(docs_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        # split docs\n",
    "        all_splits = self.text_splitter.split_documents(docs)\n",
    "\n",
    "        # Store splitted document into vector_store\n",
    "        self.vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "        # -- Create retriever\n",
    "        self.retriever = self.vector_store.as_retriever()\n",
    "\n",
    "        # set flag\n",
    "        self.is_use_rag = True\n",
    "        \n",
    "    def load_model(self):\n",
    "        # buat workflow\n",
    "        workflow = StateGraph(state_schema=State)\n",
    "\n",
    "        if self.is_use_rag:\n",
    "            workflow.add_node(\"model\", self.__generate_rag)\n",
    "        else:\n",
    "            workflow.add_node(\"model\", self.__generate)\n",
    "        \n",
    "        workflow.add_edge(START, \"model\")\n",
    "\n",
    "        # compile workflow dengan memory\n",
    "        memory = MemorySaver()\n",
    "        app = workflow.compile(checkpointer=memory)\n",
    "        return app\n",
    "\n",
    "    def __generate(self, state: State):\n",
    "        # lakukan trimmer pada history message\n",
    "        trimmed_messages = self.trimmer.invoke(state[\"messages\"])\n",
    "\n",
    "        # masukan trimmed message dan language ke prompt template \n",
    "        prompt = self.prompt_template.invoke(\n",
    "            {\"messages\": trimmed_messages}\n",
    "        )\n",
    "\n",
    "        # generate jawaban dengan model LLM\n",
    "        response = self.llm.invoke(prompt)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    def __generate_rag(self, state: State):\n",
    "        # lakukan trimmer pada history message\n",
    "        trimmed_messages = self.trimmer.invoke(state[\"messages\"])\n",
    "\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        context = self.retriever.invoke(last_message.content)\n",
    "        \n",
    "        # masukan trimmed message dan language ke prompt template \n",
    "        prompt = self.prompt_template_retrieval.invoke({\n",
    "            \"context\": context,\n",
    "            \"messages\": trimmed_messages,\n",
    "        })\n",
    "\n",
    "        # generate jawaban dengan model LLM\n",
    "        response = self.llm.invoke(prompt)\n",
    "\n",
    "        return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd75667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdvanceChatBot()\n",
    "\n",
    "# load workflow without rag\n",
    "# bot = model.load_model()\n",
    "\n",
    "# load workflow with rag\n",
    "model.load_document('../resources/ISP Company FAQ - Copy.pdf')\n",
    "bot = model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bf15fac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "reset password?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You can reset your Wi-fi password via our mobile app, website, or by contacting customer service.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "how to contact customer service?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You can contact customer service through the following methods:\n",
      "\n",
      "*   **Live Chat:** Visit our website: [https://www.example.com/contact](https://www.example.com/contact)\n",
      "*   **Call Center:** Call us at: [Insert Phone Number Here]\n",
      "*   **Email:** Send an email to: [Insert Email Address Here]\n",
      "\n",
      "Please note that the contact details might change, so itâ€™s always a good idea to check our website for the most up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "result = bot.invoke({\n",
    "    \"messages\": [HumanMessage(\"how to contact customer service?\")], \n",
    "    # \"messages\": [HumanMessage(\"hi\")], \n",
    "}, config)\n",
    "\n",
    "for chat in result[\"messages\"]:\n",
    "    chat.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b7c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biznet_test_project_2",
   "language": "python",
   "name": "biznet_test_project_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
